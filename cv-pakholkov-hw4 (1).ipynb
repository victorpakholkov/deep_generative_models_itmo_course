{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8196880,"sourceType":"datasetVersion","datasetId":4855313},{"sourceId":36696,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":30899}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/huggingface/diffusers\n%pip install git+https://github.com/huggingface/diffusers\n%pip install triton\n%pip install accelerate transformers ftfy gradio natsort safetensors\n%pip install bitsandbytes\n!pip install torch\n!pip install PIL\n!pip install xformers\n!pip install -q opencv-contrib-python\n!pip install -q controlnet_aux\n#!pip install requests\n!pip install git+https://github.com/huggingface/accelerate.git\n!wget -O civitai_midel.ckpt https://civitai.com/api/download/models/201259?type=Model&format=SafeTensor&size=pruned&fp=fp16\n!wget https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-23T10:17:17.802134Z","iopub.status.idle":"2024-04-23T10:17:17.802588Z","shell.execute_reply.started":"2024-04-23T10:17:17.802353Z","shell.execute_reply":"2024-04-23T10:17:17.802372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport requests\nfrom shutil import copyfileobj\n# from google.colab import files, drive\nfrom PIL import Image\nimport cv2\nimport numpy as np\n\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler, StableDiffusionControlNetPipeline, UniPCMultistepScheduler\nfrom diffusers.utils import load_image\nfrom IPython.display import display","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.803981Z","iopub.status.idle":"2024-04-23T10:17:17.804443Z","shell.execute_reply.started":"2024-04-23T10:17:17.804210Z","shell.execute_reply":"2024-04-23T10:17:17.804230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip list | grep torch","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.805984Z","iopub.status.idle":"2024-04-23T10:17:17.806448Z","shell.execute_reply.started":"2024-04-23T10:17:17.806210Z","shell.execute_reply":"2024-04-23T10:17:17.806229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py \\\n      --checkpoint_path='/kaggle/input/cvtai/other/1/1/epicphotogasm_zUniversal.safetensors'\\\n      --original_config_file='/kaggle/working/v1-inference.yaml'\\\n      --dump_path='/kaggle/working/cache_dir/models/civitai_model'\\\n      --scheduler_type=\"ddim\" --prediction_type='epsilon'\\\n      --from_safetensors","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.807695Z","iopub.status.idle":"2024-04-23T10:17:17.808185Z","shell.execute_reply.started":"2024-04-23T10:17:17.807915Z","shell.execute_reply":"2024-04-23T10:17:17.807935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drive.mount('/content/drive')\n# !cp -r /content/cache_dir/models/civitai_model/ /content/drive/MyDrive/\n\n''' url = 'https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora.py'\nr = requests.get(url)\nwith open('train_dreambooth_lora.py', 'w') as f:\n    f.write(r.text)\n#import train_dreambooth_lora as tl '''","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.809456Z","iopub.status.idle":"2024-04-23T10:17:17.809896Z","shell.execute_reply.started":"2024-04-23T10:17:17.809663Z","shell.execute_reply":"2024-04-23T10:17:17.809682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"if not os.path.exists('/kaggle/working/data'):\n    os.makedirs('/kaggle/working/data')\n\ndef download_image(url, destination):\n    response = requests.get(url, stream=True)\n    if response.status_code == 200:\n        with open(destination, 'wb') as file:\n            copyfileobj(response.raw, file)\n    else:\n        print(f\"Failed to download image from {url}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.811953Z","iopub.status.idle":"2024-04-23T10:17:17.812539Z","shell.execute_reply.started":"2024-04-23T10:17:17.812287Z","shell.execute_reply":"2024-04-23T10:17:17.812308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists('/kaggle/working/output_sd'):\n    os.makedirs('/kaggle/working/output_sd')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.813881Z","iopub.status.idle":"2024-04-23T10:17:17.814355Z","shell.execute_reply.started":"2024-04-23T10:17:17.814110Z","shell.execute_reply":"2024-04-23T10:17:17.814132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"urls = ['https://th.bing.com/th/id/OIP.2hBjcfycMBrbaPnL3wXu8gHaE8?rs=1&pid=ImgDetMain', 'https://th.bing.com/th/id/R.3e2bd135692331de02d86b52fcf7b275?rik=O7lPjYvnwsODwA&pid=ImgRaw&r=0', 'https://th.bing.com/th/id/R.8e1bc3aae2d737df41d876b272382eaa?rik=dKXjcqMd8KgCcQ&riu=http%3a%2f%2fwww.trussvilletribune.com%2fwp-content%2fuploads%2f2017%2f09%2fct-donald-trump-stupid-president-20170531.jpg&ehk=OhcPiDeTW%2bwYaA2bmKh1PnAX8UDY7y93P6ASzSrH%2bTE%3d&risl=&pid=ImgRaw&r=0',\n        'https://static01.nyt.com/images/2019/09/10/opinion/10bruniWeb/10bruniWeb-superJumbo.jpg?quality=90&auto=webp', 'https://th.bing.com/th/id/OIP.UfrbIgUcA723drH6oIsVkAAAAA?rs=1&pid=ImgDetMain', 'https://i0.wp.com/us-east-1.linodeobjects.com/gunaxin/2019/04/Trump.jpg?fit=1291,1291&ssl=1',\n        'https://mediaproxy.salon.com/width/1200/https://media.salon.com/2020/11/donald-trump-1123202.jpg', 'https://d.newsweek.com/en/full/1289369/president-donald-trump.jpg', 'https://th.bing.com/th/id/OIP.JxymTxgdNKMN7zf8gpLquwAAAA?rs=1&pid=ImgDetMain',\n        'https://th.bing.com/th/id/R.8ae357a671fdede9aae087afcdaef8ed?rik=zJPKrpeWjV%2fBLA&riu=http%3a%2f%2fwp.production.patheos.com%2fblogs%2fjappersandjanglers%2ffiles%2f2016%2f02%2fDonald_Trump_August_2015.jpg&ehk=kRGzsWqJUHd7a1MRaU0Ujtf%2bct2H0WzI4L8iBk5PDgo%3d&risl=1&pid=ImgRaw&r=0', 'https://static.standard.co.uk/s3fs-public/thumbnails/image/2019/09/30/11/DonaldTrump3009a.jpg', 'https://th.bing.com/th/id/R.bd0d97016599feac2964e758f696116a?rik=ar%2bgIVkQ3dUaGg&riu=http%3a%2f%2fmedia.al.com%2fnews_impact%2fphoto%2fdonald-trump-684c376f6a658f7c.jpg&ehk=%2fk9v46JmmABW1GqUtDyNfODrHfdy1tjzfIBtNGBciJw%3d&risl=&pid=ImgRaw&r=0',\n        'https://th.bing.com/th/id/R.bbd7208dd5a2ad77468829e14b135523?rik=QsZoZ%2f4tL6Ywbw&riu=http%3a%2f%2fmedia.advance.net%2fpolitics_news_national_desk%2fphoto%2fdonald-trump-b19c8bfd95bf47df.jpg&ehk=RwrCtWiKGFmqNidq4J1Oe7ZxSZG6Tfcam%2bc3UT4HAJI%3d&risl=&pid=ImgRaw&r=0', 'https://mediaproxy.salon.com/width/1200/https://media.salon.com/2016/05/donald_trump132.jpg', 'https://thesunflower.com/wp-content/uploads/2016/11/Trump45th.jpg',\n        'https://i0.wp.com/thexplorion.com/wp-content/uploads/2020/09/Donald-Trump-scaled-1.jpg?w=2560&ssl=1', 'https://th.bing.com/th/id/R.fa723c6c99a985ecea8952644358d43a?rik=jnr%2fHgyQtX7xAQ&riu=http%3a%2f%2fupload.wikimedia.org%2fwikipedia%2fcommons%2fe%2fee%2fDonald_Trump_by_Gage_Skidmore.jpg&ehk=3SX1oikep4EofGgeIxaSxhxn5jCr3G0rbN6qeMT%2bmZM%3d&risl=1&pid=ImgRaw&r=0', 'https://th.bing.com/th/id/OIP.5tI0yA0QfTUnR6bVrC2tKAHaE7?rs=1&pid=ImgDetMain',\n        'https://static.billboard.com/files/media/02-donald-trump-jan-press-conference-2017-billboard-1548-compressed.jpg', 'https://th.bing.com/th/id/OIP.EpmdjXj44XAVCfyLAOxehQAAAA?rs=1&pid=ImgDetMain', 'https://www.thefamouspeople.com/profiles/images/donald-trump-10.jpg',\n        'https://www.gannett-cdn.com/presto/2019/02/13/USAT/6b7a8c4b-11ed-416a-9c8a-c8eeca76441a-EPA_USA_TRUMP_WHITE_HOUSE_2.JPG?crop=5183,2915,x1,y259&width=3200&height=1680&fit=bounds', 'https://www.sott.net/image/s28/573063/full/president_donald_trump.jpg', 'https://th.bing.com/th/id/OIP.8Idf1Roilu2HkpPuiTcdMgAAAA?rs=1&pid=ImgDetMain',\n        'https://th.bing.com/th/id/OIP.kLDNE0qwRbBTM7QWj9j4FAAAAA?rs=1&pid=ImgDetMain', 'https://th.bing.com/th/id/R.3b90b49abb7d9c3c63ff59cae2a9f3b6?rik=LIhSRxyZNZi%2fYg&riu=http%3a%2f%2fcbsnews1.cbsistatic.com%2fhub%2fi%2f2016%2f12%2f27%2f0bdd3d20-ff41-45ef-bc71-7711a3d22540%2ftrump.jpg&ehk=WdB9LpOlukT7ceXiiyxJh0cHzYe5%2fUrFFGeJk9EeSTM%3d&risl=&pid=ImgRaw&r=0', 'https://static.politico.com/e9/06/031ab3b1406fb076ba8866a93748/150904-donald-trump-2-gty-629.jpg',\n        'https://img.washingtonpost.com/rw/2010-2019/WashingtonPost/2016/02/01/Editorial-Opinion/Images/250663066_4.jpg?uuid=WGqwrskyEeWnslovgksCyQ', 'https://d.newsweek.com/en/full/485398/donald-trump.jpg', 'https://static01.nyt.com/images/2016/10/25/us/25TRUMPTAPES1/25TRUMPTAPES1-superJumbo.jpg',\n        'https://d.newsweek.com/en/full/635470/donald-trump.jpg', 'https://img.etimg.com/photo/msid-83054761/Donald%20Trump.jpg', 'https://th.bing.com/th/id/OIP.eLn7RWUCqJi7S_3lMdu-4wAAAA?rs=1&pid=ImgDetMain',\n        'https://cloudfront-us-east-2.images.arcpublishing.com/reuters/M3WJEQDT75MHTC6OWAOB7DPAI4.jpg', 'https://th.bing.com/th/id/OIP.-gsuwTx5STfjlyGHg9_FcwHaE3?w=640&h=420&rs=1&pid=ImgDetMain', 'https://cdn.cnn.com/cnnnext/dam/assets/161128072443-01-trump-1128-super-tease.jpg',\n        'https://www.thenation.com/wp-content/uploads/2016/10/Trump_Donald_Debate_ap_img.jpg', 'https://th.bing.com/th/id/OIP.7IrIy2EAhn03waUAbInCBAHaE8?rs=1&pid=ImgDetMain', 'https://mediaproxy.salon.com/width/960/https://media.salon.com/2017/08/donald-trump60.jpg',\n        'https://th.bing.com/th/id/OIP.4wcUv4t8WIkLnSm5gLnEsgHaE8?rs=1&pid=ImgDetMain', 'https://static.independent.co.uk/s3fs-public/thumbnails/image/2017/11/23/15/donald-trump.jpg', 'https://mediaproxy.salon.com/width/1200/https://media.salon.com/2016/04/donald_trump129.jpg',\n        'https://www.thenation.com/wp-content/uploads/2017/04/Donald_Trump_CC_img.jpg?scale=896&compress=80', 'https://d.newsweek.com/en/full/694860/rtx341qy.jpg', 'https://www.michaeltyler.co.uk/wp-content/uploads/2016/03/donald-trump-e1461655495762.jpg']\n\nfor enum, url in enumerate(urls):\n    destination = os.path.join('/kaggle/working', f'pic_{enum}.jpg')\n    download_image(url, destination)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.815981Z","iopub.status.idle":"2024-04-23T10:17:17.816438Z","shell.execute_reply.started":"2024-04-23T10:17:17.816211Z","shell.execute_reply":"2024-04-23T10:17:17.816230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataroot = \"/kaggle/input/trump-brime-trimmed\"","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.817461Z","iopub.status.idle":"2024-04-23T10:17:17.817904Z","shell.execute_reply.started":"2024-04-23T10:17:17.817662Z","shell.execute_reply":"2024-04-23T10:17:17.817680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stable diffusion 1.5","metadata":{}},{"cell_type":"code","source":"if not os.path.exists('/kaggle/working/class_data_dir'):\n    os.makedirs('/kaggle/working/class_data_dir')\n\nos.environ['INSTANCE_DIR'] = \"/kaggle/input/trump-brime-trimmed\"\nos.environ['CLASS_DIR'] = \"/kaggle/working/class_data_dir\"\n\nos.environ['MODEL_NAME'] = \"/kaggle/working/cache_dir/models/civitai_model/\"\nos.environ['OUTPUT_DIR'] = \"/kaggle/working/output_sd\"","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.819235Z","iopub.status.idle":"2024-04-23T10:17:17.819684Z","shell.execute_reply.started":"2024-04-23T10:17:17.819463Z","shell.execute_reply":"2024-04-23T10:17:17.819484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 /kaggle/working/diffusers/examples/dreambooth/train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --instance_prompt=\"a photo of sks man face\" \\\n  --class_prompt=\"a photo of man face \" \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=2e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --gradient_accumulation_steps=1 \\\n  --num_class_images=500 \\\n  --max_train_steps=800 \\\n  --checkpointing_steps=800 \\\n  --use_8bit_adam \\\n  --mixed_precision=\"no\"\\\n  --train_text_encoder","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.820612Z","iopub.status.idle":"2024-04-23T10:17:17.821059Z","shell.execute_reply.started":"2024-04-23T10:17:17.820822Z","shell.execute_reply":"2024-04-23T10:17:17.820843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = os.environ['OUTPUT_DIR']\n\npipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\ng_cuda=None","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.822775Z","iopub.status.idle":"2024-04-23T10:17:17.823241Z","shell.execute_reply.started":"2024-04-23T10:17:17.822989Z","shell.execute_reply":"2024-04-23T10:17:17.823008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g_cuda = torch.Generator(device='cuda')\nseed = 345252\ng_cuda.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.824808Z","iopub.status.idle":"2024-04-23T10:17:17.825337Z","shell.execute_reply.started":"2024-04-23T10:17:17.825034Z","shell.execute_reply":"2024-04-23T10:17:17.825055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"portrait of sks man face, on the street, lights, midnight, NY, standing, 4K, raw, hrd, hd, high quality, realism, sharp focus,  sarcastic eyes, detailed eyes, ironic expression\"\nnegative_prompt = \"deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands disconnected limbs, mutation, ugly, blurry, amputation\"\nnum_samples = 2\nguidance_scale = 7.5\nnum_inference_steps = 35\nheight = 768\nwidth = 1024\n\nwith autocast(\"cuda\"), torch.inference_mode():\n    images = pipe(\n        prompt,\n        height=height,\n        width=width,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_samples,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        generator=g_cuda\n    ).images\n\nfor img in images:\n    display(img)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.826903Z","iopub.status.idle":"2024-04-23T10:17:17.827402Z","shell.execute_reply.started":"2024-04-23T10:17:17.827132Z","shell.execute_reply":"2024-04-23T10:17:17.827152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token = \"sks\"\npromt_list = [\n    {\n     \"name\": \"kitchen\",\n     \"prompt\":f\"close up portrait of {token} man face, in the kitchen, standing, 4K, raw, hrd, hd, high quality, realism, sharp focus\",\n     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands disconnected limbs, mutation, ugly, blurry, amputation\",\n    },\n    {\n     \"name\": \"forest\",\n     \"prompt\":f\"portrait of {token} man face, in the forest, standing, 4K, raw, hrd, hd, high quality, realism, sharp focus\",\n     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands disconnected limbs, mutation, ugly, blurry, amputation\",\n    },\n    {\n     \"name\": \"street\",\n     \"prompt\":f\"portrait of {token} man face, on the street, lights, midnight, NY, standing, 4K, raw, hrd, hd, high quality, realism, sharp focus,  beautiful eyes, detailed eyes\",\n     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands, mutation, ugly, blurry\",\n    },\n    {\n     \"name\": \"anime\",\n     \"prompt\":f\"portrait of {token} man face, anime style, japan, midnight, Tokyo, standing, 4K, raw, hrd, hd, high quality, sharp focus, detailed eyes\",\n     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands, mutation, ugly, blurry\",\n    },\n    {\n     \"name\": \"classicist art\",\n     \"prompt\":f\"portrait of {token} man face, classic art style, medieval europe, moonlight, standing, 4K, raw, hrd, hd, high quality, sharp focus, detailed eyes\",\n     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands, mutation, ugly, blurry\",\n    },\n]\n\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.829019Z","iopub.status.idle":"2024-04-23T10:17:17.829486Z","shell.execute_reply.started":"2024-04-23T10:17:17.829249Z","shell.execute_reply":"2024-04-23T10:17:17.829276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 1666\nrepeat = 2\nnum_samples = 2\nguidance_scale = 7.5\nnum_inference_steps = 30\nheight = 768\nwidth = 1024\n\n\nif not os.path.exists('/kaggle/working/output_pics_sd'):\n    os.makedirs('/kaggle/working/output_pics_sd')\n\nsave_folder = \"/kaggle/working/output_pics_sd\"\nsave_mode = \"with_train_token\"\n# save_mode = \"base_model\"","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:54.227741Z","iopub.execute_input":"2024-04-23T10:17:54.228139Z","iopub.status.idle":"2024-04-23T10:17:54.234573Z","shell.execute_reply.started":"2024-04-23T10:17:54.228108Z","shell.execute_reply":"2024-04-23T10:17:54.233614Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"for idx, sample in enumerate(promt_list):\n    prompt = sample.get(\"prompt\")\n    negative_prompt = sample.get(\"n_prompt\")\n    name = sample.get(\"name\")\n    image_list = []\n    for _ in range(repeat):\n        generator = g_cuda\n        with autocast(\"cuda\"), torch.inference_mode():\n            images = pipe(\n                prompt,\n                height=height,\n                width=width,\n                negative_prompt=negative_prompt,\n                num_images_per_prompt=num_samples,\n                num_inference_steps=num_inference_steps,\n                guidance_scale=guidance_scale,\n                generator=generator\n            ).images\n        image_list.extend(images)\n        seed+=345324\n    img_grid = image_grid(image_list, num_samples, repeat)\n    save_path = os.path.join(save_folder, save_mode, f\"{height}x{width}\")\n    os.makedirs(save_path, exist_ok=True)\n    img_grid.save(os.path.join(save_path, f\"{name}.jpg\"))","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:18:00.752105Z","iopub.execute_input":"2024-04-23T10:18:00.753005Z","iopub.status.idle":"2024-04-23T10:29:18.113360Z","shell.execute_reply.started":"2024-04-23T10:18:00.752968Z","shell.execute_reply":"2024-04-23T10:29:18.112499Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19a29b9bf347413bb26c019187bfa05c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95be7b5532d54410ace87f00261908f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"540174d3c2ec4b5d803b654e4abff908"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16bff58b96834c9d8d72a4d7080b689b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdbdc4f3986b4f8c8f0cffe90e5114d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7683eed17c504914a2806b67f5a4dbb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d12d71bfeaf2484eab94c9890d4ee43c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be39aaa2aa5740e69178fb759822078d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ae0d6ff8a3d459ea5754e6e214e72b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2287652fd4d49eda770387210b3ecd9"}},"metadata":{}}]},{"cell_type":"raw","source":"## Lora","metadata":{}},{"cell_type":"code","source":"%pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.834168Z","iopub.status.idle":"2024-04-23T10:17:17.834626Z","shell.execute_reply.started":"2024-04-23T10:17:17.834383Z","shell.execute_reply":"2024-04-23T10:17:17.834402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists('/kaggle/working/class_data_lora'):\n    os.makedirs('/kaggle/working/class_data_lora')\n\nif not os.path.exists('/kaggle/working/output_lora_1'):\n    os.makedirs('/kaggle/working/output_lora_1')\nif not os.path.exists('/kaggle/working/output_lora_2'):\n    os.makedirs('/kaggle/working/output_lora_2')\nif not os.path.exists('/kaggle/working/output_lora_3'):\n    os.makedirs('/kaggle/working/output_lora_3')\n\n# os.environ['INSTANCE_DIR'] = \"/kaggle/working/data_cr\"\nos.environ['CLASS_DIR_LORA'] = \"/kaggle/working/class_data_lora\"\n\n# os.environ['MODEL_NAME'] = \"/kaggle/working/cache_dir/models/civitai_model/\"\nos.environ['OUTPUT_DIR_LORA_1'] = \"/kaggle/working/output_lora_1\"\nos.environ['OUTPUT_DIR_LORA_2'] = \"/kaggle/working/output_lora_2\"\nos.environ['OUTPUT_DIR_LORA_3'] = \"/kaggle/working/output_lora_3\"","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.836435Z","iopub.status.idle":"2024-04-23T10:17:17.836887Z","shell.execute_reply.started":"2024-04-23T10:17:17.836641Z","shell.execute_reply":"2024-04-23T10:17:17.836659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 /kaggle/working/diffusers/examples/dreambooth/train_dreambooth_lora.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR_LORA_1 \\\n  --instance_prompt=\"a photo of sks man face\" \\\n  --class_prompt=\"a photo of man face \" \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --seed=1111 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=5e-4 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --gradient_accumulation_steps=1 \\\n  --num_class_images=500 \\\n  --max_train_steps=800 \\\n  --checkpointing_steps=800 \\\n  --use_8bit_adam \\\n  --mixed_precision=\"no\"\\\n  --train_text_encoder \\\n  --rank=4","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.838552Z","iopub.status.idle":"2024-04-23T10:17:17.839014Z","shell.execute_reply.started":"2024-04-23T10:17:17.838783Z","shell.execute_reply":"2024-04-23T10:17:17.838804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 /kaggle/working/diffusers/examples/dreambooth/train_dreambooth_lora.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR_LORA_2 \\\n  --instance_prompt=\"a photo of sks man face\" \\\n  --class_prompt=\"a photo of man face \" \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --seed=1111 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=5e-4 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --gradient_accumulation_steps=1 \\\n  --num_class_images=500 \\\n  --max_train_steps=800 \\\n  --checkpointing_steps=800 \\\n  --use_8bit_adam \\\n  --mixed_precision=\"no\"\\\n  --train_text_encoder \\\n  --rank=8","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.840132Z","iopub.status.idle":"2024-04-23T10:17:17.840585Z","shell.execute_reply.started":"2024-04-23T10:17:17.840355Z","shell.execute_reply":"2024-04-23T10:17:17.840374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 /kaggle/working/diffusers/examples/dreambooth/train_dreambooth_lora.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR_LORA_3 \\\n  --instance_prompt=\"a photo of sks man face\" \\\n  --class_prompt=\"a photo of man face \" \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --seed=1111 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=5e-4 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --gradient_accumulation_steps=1 \\\n  --num_class_images=500 \\\n  --max_train_steps=800 \\\n  --checkpointing_steps=800 \\\n  --use_8bit_adam \\\n  --mixed_precision=\"no\"\\\n  --train_text_encoder \\\n  --rank=24","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.841833Z","iopub.status.idle":"2024-04-23T10:17:17.842312Z","shell.execute_reply.started":"2024-04-23T10:17:17.842046Z","shell.execute_reply":"2024-04-23T10:17:17.842064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lora_infer(model_path, weights, save_folder):\n    pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n    pipe.load_lora_weights(weights)\n    g_cuda=None\n    \n    if not os.path.exists(save_folder):\n        os.makedirs(save_folder)\n    save_folder = save_folder\n    save_mode = \"base_model\"\n    for idx, sample in enumerate(promt_list):\n    prompt = sample.get(\"prompt\")\n    negative_prompt = sample.get(\"n_prompt\")\n    name = sample.get(\"name\")\n    image_list = []\n    for _ in range(repeat):\n        generator = torch.Generator(\"cuda\").manual_seed(seed)\n        with autocast(\"cuda\"), torch.inference_mode():\n            images = pipe(\n                prompt,\n                height=height,\n                width=width,\n                negative_prompt=negative_prompt,\n                num_images_per_prompt=num_samples,\n                num_inference_steps=num_inference_steps,\n                guidance_scale=guidance_scale,\n                generator=generator\n            ).images\n        image_list.extend(images)\n        seed+=345324","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.843890Z","iopub.status.idle":"2024-04-23T10:17:17.844394Z","shell.execute_reply.started":"2024-04-23T10:17:17.844130Z","shell.execute_reply":"2024-04-23T10:17:17.844152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path1 = os.environ['OUTPUT_DIR_LORA_1']\nmodel_path2 = os.environ['OUTPUT_DIR_LORA_2']\nmodel_path3 = os.environ['OUTPUT_DIR_LORA_3']\n\nweights1 = \"/kaggle/working/output_lora_1/diffusion_pytorch_lora.bin\"\nweights2 = \"/kaggle/working/output_lora_2/diffusion_pytorch_lora.bin\"\nweights3 = \"/kaggle/working/output_lora_3/diffusion_pytorch_lora.bin\"\n\nsave_folder1 = '/content/output_pics_lora_rank_4'\nsave_folder2 = '/content/output_pics_lora_rank_8'\nsave_folder3 = '/content/output_pics_lora_rank_24'\n\n\n\nlora_infer(model_path1, weights1, save_folder1)\nlora_infer(model_path2, weights2, save_folder2)\nlora_infer(model_path3, weights3, save_folder3)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:17:17.856487Z","iopub.status.idle":"2024-04-23T10:17:17.856966Z","shell.execute_reply.started":"2024-04-23T10:17:17.856698Z","shell.execute_reply":"2024-04-23T10:17:17.856728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unet vs. Lora comparison","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ControlNet","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}